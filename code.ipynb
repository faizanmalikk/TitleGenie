{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(2)\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "#load all the datasets \n",
    "df1 = pd.read_csv('USvideos.csv')\n",
    "df2 = pd.read_csv('CAvideos.csv')\n",
    "df3 = pd.read_csv('GBvideos.csv')\n",
    "\n",
    "#load the datasets containing the category names\n",
    "data1 = json.load(open('US_category_id.json'))\n",
    "data2 = json.load(open('CA_category_id.json'))\n",
    "data3 = json.load(open('GB_category_id.json'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to process our data so that we can use this data to train our model for the task of title generator. Here are all the data cleaning and processing steps that we need to follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_extractor(data):\n",
    "    i_d = [data['items'][i]['id'] for i in range(len(data['items']))]\n",
    "    title = [data['items'][i]['snippet'][\"title\"] for i in range(len(data['items']))]\n",
    "    i_d = list(map(int, i_d))\n",
    "    category = zip(i_d, title)\n",
    "    category = dict(category)\n",
    "    return category\n",
    "\n",
    "#create a new category column by mapping the category names to their id\n",
    "df1['category_title'] = df1['category_id'].map(category_extractor(data1))\n",
    "df2['category_title'] = df2['category_id'].map(category_extractor(data2))\n",
    "df3['category_title'] = df3['category_id'].map(category_extractor(data3))\n",
    "\n",
    "#join the dataframes\n",
    "df = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "\n",
    "#drop rows based on duplicate videos\n",
    "df = df.drop_duplicates('video_id')\n",
    "\n",
    "#collect only titles of entertainment videos\n",
    "#feel free to use any category of video that you want\n",
    "entertainment = df[df['category_title'] == 'Entertainment']['title']\n",
    "entertainment = entertainment.tolist()\n",
    "\n",
    "\n",
    "\n",
    "#remove punctuations and convert text to lowercase\n",
    "def clean_text(text):\n",
    "    text = ''.join(e for e in text if e not in string.punctuation).lower()\n",
    "    \n",
    "    text = text.encode('utf8').decode('ascii', 'ignore')\n",
    "    return text\n",
    "\n",
    "corpus = [clean_text(e) for e in entertainment]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natural language processing tasks require input data in the form of a sequence of tokens. The first step after data cleansing is to generate a sequence of n-gram tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    # Get tokens\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "    # Convert to sequence of tokens\n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i + 1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "\n",
    "    return input_sequences, total_words\n",
    "\n",
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding the sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the sequences can be of variable length, the sequence lengths must be equal. When using neural networks, we usually feed an input into the network while waiting for output. In practice, it is more efficient to process data in batches rather than one at a time.\n",
    "\n",
    "I will create sequences of n-gram as predictors and the following word of n-gram as label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.utils as ku  # Ensure this line is present at the top of your code\n",
    "\n",
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    predictors, label = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "# Call the function to generate predictors and labels\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will use the LSTM Model to build a model for the task of Title Generator with Machine Learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1  # Use the correct minus sign\n",
    "    model = Sequential()\n",
    " \n",
    "    # Add Input Embedding Layer\n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    " \n",
    "    # Add Hidden Layer 1 — LSTM Layer\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.1))\n",
    " \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, activation='softmax'))  # Use regular quotes\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')  # Use regular quotes\n",
    " \n",
    "    return model\n",
    "\n",
    "# Create and fit the model\n",
    "model = create_model(max_sequence_len, total_words)\n",
    "model.fit(predictors, label, epochs=5, verbose=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as we have created a function to generate titles let’s test our title generator model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(“spiderman”, 5, model, max_sequence_len))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
